{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">Detail understanding of HMR paper</div>\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "you can Find me on Github:\n",
    "> ###### [ GitHub](https://github.com/lev1khachatryan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HMR is an end-to-end framework for recovering a full 3D mesh of a human body from a single RGB image. It uses the ***generative human body*** model, ***SMPL***, which parameterizes the mesh by 3D joint angles and a low-dimensional linear shape space.\n",
    "\n",
    "The output is holistic - always infer the full 3D body even in case of ***occlusion*** and ***truncation***.\n",
    "\n",
    "In 3D analysis most approaches focus on recovering 3D joint locations which alone are not constrain the full DoF (degree of freedom) at each joint. This means that it is non-trivial to estimate the full pose of the body from only the 3D joint locations. \n",
    "\n",
    "In contrast, HMR outputs the ***relative 3D rotation matrices for each joint in the kinematic tree***, capturing information about 3D head and limb orientation. Predicting rotations also ensures that limbs are symmetric and of valid length. Model implicitly learns the joint angle limits from datasets of 3D body models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During training we assume that all images are annotated with ground truth 2D joints. We also consider the case in which some have 3D annotations as well. Additionally we assume that there is a pool of 3D meshes of human bodies of varying shape and pose. Since these meshes do not necessarily have a corresponding image, we refer to this data as unpaired.\n",
    "\n",
    "Figure 2 shows the overview of the proposed network architecture, which can be trained end-to-end. Convolutional features of the image are sent to the iterative 3D regression module whose objective is to infer the 3D human body and the camera such that its 3D joints project onto the annotated 2D joints. The inferred parameters are also sent to an adversarial discriminator network whose task is to determine if the 3D parameters are real meshes from the unpaired data. This encourages the network to output 3D human bodies that lie on the manifold of human bodies and acts as a weaksupervision for in-the-wild images without ground truth 3D annotations. Due to the rich representation of the 3D mesh model, this data-driven prior can capture joint angle limits, anthropometric constraints (e.g. height, weight, bone ratios), and subsumes the geometric priors used by models that only predict 3D joint locations. When ground truth 3D information is available, we may use it as an intermediate loss. In all, our overall objective is:\n",
    "\n",
    "$L=\\lambda(L_{reproj} + 1L_{3D}) + L_{adv}$\n",
    "\n",
    "where $\\lambda$ controls the relative importance of each objective, $1$ is an indicator function that is 1 if ground truth 3D is available for an image and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='_assets/HMR/HMR.png'>\n",
    "\n",
    "**Figure 2**: Overview of the proposed framework. An image I is passed through a convolutional encoder. This is sent to\n",
    "an iterative 3D regression module that infers the latent 3D representation of the human that minimizes the joint reprojection\n",
    "error. The 3D parameters are also sent to the discriminator D, whose goal is to tell if these parameters come from a real\n",
    "human shape and pose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D Body Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the 3D mesh of a human body using the Skinned Multi-Person Linear (SMPL) model. SMPL is a generative model that factors human bodies into ***shape*** (how individuals vary in height, weight, body proportions) and ***pose*** (how the 3D surface deforms with articulation).\n",
    "\n",
    "The shape $\\beta \\in R^{10}$ is parameterized by the first 10 coefs of a PCA shape space. The pose $\\theta \\in R^{3K}$ is modeled by relative 3D rotation of K = 23 joints in axis-angle representation.\n",
    "\n",
    "SMPL is a differentiable function that outputs a triangulated mesh with N = 6980 vertices, $M(\\theta, \\beta) \\in R^{3xN}$, which is obtained by shaping the template body vertices conditioned on $\\beta$ and $\\theta$, then articulating the bones according to the joint rotations $\\theta$ via ***forward kinematics***, and finally deforming the surface with ***linear blend skinning***. The 3D keypoints used for reprojection error, $X(\\theta, \\beta) \\in R^{3xP}$, are obtained by linear regression from the final mesh vertices.\n",
    "\n",
    "We employ the weak-perspective camera model and solve for the global rotation $R \\in R^{3x3}$ in axis-angle representation, translation $t \\in R^2$ and scale $s \\in R$. Thus the set of parameters that represent the 3D reconstruction of a human body is expressed as a 85 dimensional vector $\\Theta = \\{ \\theta, \\beta, R, t, s \\}$. Given $\\Theta$, the projection of $X(\\theta, \\beta)$ is $\\hat{x}=s \\Pi(RX(\\theta, \\beta)) + t$  where $\\Pi$ is an orthographic projection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verts, Js, pred_Rs = smpl(shapes, poses, get_skin=True)\n",
    "pred_Rs = tf.reshape(pred_Rs, [-1, 24, 9])\n",
    "\n",
    "print('Shape of vertices - verts: ', verts.shape)\n",
    "print('Shape of Joints - Js: ', Js.shape)\n",
    "print('Shape of rotation matrices of poses - pred_Rs: ', pred_Rs.shape)\n",
    "\n",
    "# Shape of vertices - verts:  (1, 6890, 3)\n",
    "# Shape of Joints - Js:  (1, 19, 3)\n",
    "# Shape of rotation matrices of poses - pred_Rs:  (1, 24, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_orth_proj_idrot(X, camera, name=None):\n",
    "    \"\"\"\n",
    "    X is N x num_points x 3\n",
    "    camera is N x 3\n",
    "    same as applying orth_proj_idrot to each N \n",
    "    \"\"\"\n",
    "    with tf.name_scope(name, \"batch_orth_proj_idrot\", [X, camera]):\n",
    "        \n",
    "        # reshape to (1, 1, 3)\n",
    "        camera = tf.reshape(camera, [-1, 1, 3], name=\"cam_adj_shape\")\n",
    "\n",
    "        # (1, 19, 2) + (1, 1, 2) = (1, 19, 2)\n",
    "        X_trans = X[:, :, :2] + camera[:, :, 1:]\n",
    "\n",
    "        shape = tf.shape(X_trans)\n",
    "        \n",
    "        # camera[:, :, 0] is (1, 1)\n",
    "        # tf.reshape(X_trans, [shape[0], -1]) is (1, 38)\n",
    "        # camera[:, :, 0] * tf.reshape(X_trans, [shape[0], -1]) is (1, 38)\n",
    "        return tf.reshape(\n",
    "            camera[:, :, 0] * tf.reshape(X_trans, [shape[0], -1]), shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_kp = batch_orth_proj_idrot(Js, cams, name='proj2d_stage%d' % i)\n",
    "\n",
    "print('Shape of camera - cams: ', cams.shape)\n",
    "print('Shape of predicted key-points - pred_kp: ', pred_kp.shape)\n",
    "\n",
    "# Shape of camera - cams:  (1, 3)\n",
    "# Shape of predicted key-points - pred_kp:  (1, 19, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative 3D Regression with Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of the 3D regression module is to output $\\Theta$ given an image encoding $\\phi$ such that the joint reprojection error $L_{reproj}=\\sum_{i}||v_i(x_i - \\hat{x_i})||_1$ is minimized. Here $v_i \\in \\{0, 1\\}^K$ is the visibility for each of the K joints. However, directly regressing $\\Theta$ in one go is a challenging task, particularly because $\\Theta$ includes rotation parameters. In this work, we regress $\\Theta$ in an iterative error feedback (IEF) loop, where progressive changes are made recurrently to the current estimate. \n",
    "\n",
    "Specifically, the 3D regression module takes the image features $\\phi$ and the current parameters $\\Theta_t$ as an input and outputs the residual $\\Delta \\Theta_t$. The parameter is updated by adding this residual to the current estimate $\\Theta_{t+1} = \\Theta_t + \\Delta \\Theta_t$. The initial estimate $\\Theta_0$ is set as the mean $\\hat{\\Theta}$. In this work, we keep everything in the latent space and simply concatenate the features $[\\phi, \\Theta]$ as the input to the regressor. We find that\n",
    "this works well and is suitable when differentiable rendering of the parameters is non-trivial.\n",
    "\n",
    "Additional direct 3D supervision may be employed when paired ground truth 3D data is available. The most common form of 3D annotation is the 3D joints. Below are the definitions of the 3D losses.\n",
    "\n",
    "$L_{3D}=L_{3D joints} + L_{3D smpl}$\n",
    "\n",
    "$L_{joints}=||(X_i - \\hat{X_i})||_2^2$\n",
    "\n",
    "$L_{smpl}=||[\\beta_i, \\theta_i] - [\\hat{\\beta_i}, \\hat{\\theta_i}]||_2^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape of encoded image - img_feat: ', img_feat.shape)\n",
    "# Shape of encoded image - img_feat:  (1, 2048)\n",
    "\n",
    "# loads mean params from neutral_smpl_mean_params.h5 pretrained model\n",
    "theta_prev = load_mean_param()\n",
    "\n",
    "print('Shape of initial Theta - theta_prev: ', theta_prev.shape)\n",
    "# Shape of initial Theta - theta_prev:  (1, 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main IEF loop, num_stage is by default 3\n",
    "for i in np.arange(num_stage):\n",
    "    # (1, 2133)\n",
    "    state = tf.concat([img_feat, theta_prev], 1)\n",
    "    \n",
    "    # threed_enc_fn is 3 layered MLP (last is the output)\n",
    "    delta_theta, _ = threed_enc_fn(\n",
    "        state, num_output=85, reuse=True)\n",
    "\n",
    "    # Compute new theta\n",
    "    theta_here = theta_prev + delta_theta\n",
    "    # cam = N x 3, pose N x self.num_theta, shape: N x 10\n",
    "    cams = theta_here[:, :3]\n",
    "    poses = theta_here[:, 3:(3 + 24*3)]\n",
    "    shapes = theta_here[:, (3 + 24*3):]\n",
    "    # pred_Rs is Nx24x3x3 rotation matrices of poses\n",
    "    verts, Js, pred_Rs = self.smpl(shapes, poses, get_skin=True)\n",
    "    pred_kp = batch_orth_proj_idrot(\n",
    "        Js, cams, name='proj2d_stage%d' % i)\n",
    "    # --- Compute losses: loss_kps is initially empty list\n",
    "    loss_kps.append(keypoint_loss(kp_loader, pred_kp))\n",
    "    pred_Rs = tf.reshape(pred_Rs, [-1, 24, 9])\n",
    "    if self.use_3d_label:\n",
    "        loss_poseshape, loss_joints = get_3d_loss(pred_Rs, shapes, Js)\n",
    "        loss_3d_params.append(loss_poseshape)\n",
    "        loss_3d_joints.append(loss_joints)\n",
    "\n",
    "    # Save pred_rotations for Discriminator: \n",
    "    # fake_rotations and fake_shapes are initially empty lists\n",
    "    fake_rotations.append(pred_Rs[:, 1:, :])\n",
    "    fake_shapes.append(shapes)\n",
    "\n",
    "    theta_prev = theta_here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoint_loss(kp_gt, kp_pred, scale=1., name=None):\n",
    "    \"\"\"\n",
    "    computes: \\Sum_i [0.5 * vis[i] * |kp_gt[i] - kp_pred[i]|] / (|vis|)\n",
    "    Inputs:\n",
    "      kp_gt  : N x K x 3\n",
    "      kp_pred: N x K x 2\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name, \"keypoint_l1_loss\", [kp_gt, kp_pred]):\n",
    "        kp_gt = tf.reshape(kp_gt, (-1, 3))\n",
    "        kp_pred = tf.reshape(kp_pred, (-1, 2))\n",
    "\n",
    "        vis = tf.expand_dims(tf.cast(kp_gt[:, 2], tf.float32), 1)\n",
    "        res = tf.losses.absolute_difference(kp_gt[:, :2], kp_pred, weights=vis)\n",
    "        return res\n",
    "    \n",
    "def compute_3d_loss(params_pred, params_gt, has_gt3d):\n",
    "    \"\"\"\n",
    "    Computes the l2 loss between 3D params pred and gt for those data that has_gt3d is True.\n",
    "    Parameters to compute loss over:\n",
    "    3Djoints: 14*3 = 42\n",
    "    rotations:(24*9)= 216\n",
    "    shape: 10\n",
    "    total input: 226 (gt SMPL params) or 42 (just joints)\n",
    "\n",
    "    Inputs:\n",
    "      params_pred: N x {226, 42}\n",
    "      params_gt: N x {226, 42}\n",
    "      # has_gt3d: (N,) bool\n",
    "      has_gt3d: N x 1 tf.float32 of {0., 1.}\n",
    "    \"\"\"\n",
    "    with tf.name_scope(\"3d_loss\", [params_pred, params_gt, has_gt3d]):\n",
    "        weights = tf.expand_dims(tf.cast(has_gt3d, tf.float32), 1)\n",
    "        res = tf.losses.mean_squared_error(\n",
    "            params_gt, params_pred, weights=weights) * 0.5\n",
    "        return res\n",
    "def get_3d_loss(Rs, shape, Js):\n",
    "    \"\"\"\n",
    "    Rs is N x 24 x 3*3 rotation matrices of pose\n",
    "    Shape is N x 10\n",
    "    Js is N x 19 x 3 joints\n",
    "\n",
    "    Ground truth:\n",
    "    self.poseshape_loader is a long vector of:\n",
    "       relative rotation (24*9)\n",
    "       shape (10)\n",
    "       3D joints (14*3)\n",
    "    \"\"\"\n",
    "    Rs = tf.reshape(Rs, [self.batch_size, -1])\n",
    "    params_pred = tf.concat([Rs, shape], 1, name=\"prep_params_pred\")\n",
    "    # 24*9+10 = 226\n",
    "    gt_params = self.poseshape_loader[:, :226]\n",
    "    loss_poseshape = self.e_3d_weight * compute_3d_loss(\n",
    "        params_pred, gt_params, self.has_gt3d_smpl)\n",
    "    # 14*3 = 42\n",
    "    gt_joints = self.poseshape_loader[:, 226:]\n",
    "    pred_joints = Js[:, :14, :]\n",
    "    # Align the joints by pelvis.\n",
    "    pred_joints = align_by_pelvis(pred_joints)\n",
    "    pred_joints = tf.reshape(pred_joints, [self.batch_size, -1])\n",
    "    gt_joints = tf.reshape(gt_joints, [self.batch_size, 14, 3])\n",
    "    gt_joints = align_by_pelvis(gt_joints)\n",
    "    gt_joints = tf.reshape(gt_joints, [self.batch_size, -1])\n",
    "\n",
    "    loss_joints = self.e_3d_weight * compute_3d_loss(\n",
    "        pred_joints, gt_joints, self.has_gt3d_joints)\n",
    "\n",
    "    return loss_poseshape, loss_joints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorized Adversarial Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reprojection loss encourages the network to produce a 3D body that explains the 2D joint locations, however  nthropometrically implausible 3D bodies or bodies with gross self-intersections may still minimize the reprojection loss. To regularize this, we use a discriminator network D that is trained to tell whether SMPL parameters correspond to a real body or not. We refer to this as an adversarial prior since the discriminator acts as a data-driven prior that guides the 3D inference.\n",
    "\n",
    "A further benefit of employing a rich, explicit 3D representation like SMPL is that we precisely know the meaning of the latent space. In particular SMPL has a factorized form that we can take advantage of to make the adversary more data efficient and stable to train. More concretely, we mirror the shape and pose decomposition of SMPL and train a discriminator for shape and pose independently. The pose is based on a kinematic tree, so we further decompose the pose discriminators and train one for each joint rotation. This amounts to learning the angle limits for each joint. In order to capture the joint distribution of the entire kinematic tree, we also learn a discriminator that takes in all the rotations.\n",
    "\n",
    "Since the input to each discriminator is very low dimensional (10-D for $\\beta$, 9-D for each joint and 9K-D for all joints), they can each be small networks, making them rather stable to train. All pose discriminators share a common feature space of rotation matrices and only the final classifiers are learned separately.\n",
    "\n",
    "In all we train K + 2 discriminators. Each discriminator $D_{i}$ outputs values between [0, 1], representing the probability that $\\Theta$ came from the data. In practice we use the least square formulation for its stability. Let E represent the\n",
    "encoder including the image encoder and the 3D module.\n",
    "\n",
    "Then the adversarial loss function for the encoder is: $min L_{adv}(E) \\sum_{i}E_{\\Theta \\sim p_E}[(D_i(E(I))-1)^2]$\n",
    "\n",
    "and the objective for each discriminator is $min L(D_i) = E_{\\Theta \\sim p_{data}}[(D_i(\\Theta)-1)^2] + E_{\\Theta \\sim p_E}[D_i(E(I))^2]$\n",
    "\n",
    "We optimize $E$ and all $D_{i}$'s jointly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_discriminator(self, fake_rotations, fake_shapes):\n",
    "    # Compute the rotation matrices of \"real\" pose\n",
    "    # These guys are in 24 x 3.\n",
    "    real_rotations = batch_rodrigues(tf.reshape(self.pose_loader, [-1, 3]))\n",
    "    real_rotations = tf.reshape(real_rotations, [-1, 24, 9])\n",
    "    # Ignoring global rotation. N x 23*9\n",
    "    # The # of real rotation is B*num_stage so it's balanced.\n",
    "    real_rotations = real_rotations[:, 1:, :]\n",
    "    all_fake_rotations = tf.reshape(\n",
    "        tf.concat(fake_rotations, 0),\n",
    "        [self.batch_size * self.num_stage, -1, 9])\n",
    "    comb_rotations = tf.concat(\n",
    "        [real_rotations, all_fake_rotations], 0, name=\"combined_pose\")\n",
    "\n",
    "    comb_rotations = tf.expand_dims(comb_rotations, 2)\n",
    "    all_fake_shapes = tf.concat(fake_shapes, 0)\n",
    "    comb_shapes = tf.concat(\n",
    "        [self.shape_loader, all_fake_shapes], 0, name=\"combined_shape\")\n",
    "\n",
    "    disc_input = {\n",
    "        'weight_decay': self.d_wd,\n",
    "        'shapes': comb_shapes,\n",
    "        'poses': comb_rotations\n",
    "    }\n",
    "\n",
    "    self.d_out, self.D_var = Discriminator_separable_rotations(\n",
    "        **disc_input)\n",
    "\n",
    "    self.d_out_real, self.d_out_fake = tf.split(self.d_out, 2)\n",
    "    # Compute losses:\n",
    "    with tf.name_scope(\"comp_d_loss\"):\n",
    "        self.d_loss_real = tf.reduce_mean(\n",
    "            tf.reduce_sum((self.d_out_real - 1)**2, axis=1))\n",
    "        self.d_loss_fake = tf.reduce_mean(\n",
    "            tf.reduce_sum((self.d_out_fake)**2, axis=1))\n",
    "        # Encoder loss\n",
    "        self.e_loss_disc = tf.reduce_mean(\n",
    "            tf.reduce_sum((self.d_out_fake - 1)**2, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_rodrigues(theta, name=None):\n",
    "    \"\"\"\n",
    "    Theta is N x 3\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name, \"batch_rodrigues\", [theta]):\n",
    "        batch_size = theta.shape.as_list()[0]\n",
    "        \n",
    "        angle = tf.expand_dims(tf.norm(theta + 1e-8, axis=1), -1)\n",
    "        r = tf.expand_dims(tf.div(theta, angle), -1)\n",
    "\n",
    "        angle = tf.expand_dims(angle, -1)\n",
    "        cos = tf.cos(angle)\n",
    "        sin = tf.sin(angle)\n",
    "\n",
    "        outer = tf.matmul(r, r, transpose_b=True, name=\"outer\")\n",
    "\n",
    "        eyes = tf.tile(tf.expand_dims(tf.eye(3), 0), [batch_size, 1, 1])\n",
    "        R = cos * eyes + (1 - cos) * outer + sin * batch_skew(\n",
    "            r, batch_size=batch_size)\n",
    "        return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator_separable_rotations(\n",
    "        poses,\n",
    "        shapes,\n",
    "        weight_decay,\n",
    "):\n",
    "    \"\"\"\n",
    "    23 Discriminators on each joint + 1 for all joints + 1 for shape.\n",
    "    To share the params on rotations, this treats the 23 rotation matrices\n",
    "    as a \"vertical image\":\n",
    "    Do 1x1 conv, then send off to 23 independent classifiers.\n",
    "\n",
    "    Input:\n",
    "    - poses: N x 23 x 1 x 9, NHWC ALWAYS!!\n",
    "    - shapes: N x 10\n",
    "    - weight_decay: float\n",
    "\n",
    "    Outputs:\n",
    "    - prediction: N x (1+23) or N x (1+23+1) if do_joint is on.\n",
    "    - variables: tf variables\n",
    "    \"\"\"\n",
    "    data_format = \"NHWC\"\n",
    "    with tf.name_scope(\"Discriminator_sep_rotations\", [poses, shapes]):\n",
    "        with tf.variable_scope(\"D\") as scope:\n",
    "            with slim.arg_scope(\n",
    "                [slim.conv2d, slim.fully_connected],\n",
    "                    weights_regularizer=slim.l2_regularizer(weight_decay)):\n",
    "                with slim.arg_scope([slim.conv2d], data_format=data_format):\n",
    "                    poses = slim.conv2d(poses, 32, [1, 1], scope='D_conv1')\n",
    "                    poses = slim.conv2d(poses, 32, [1, 1], scope='D_conv2')\n",
    "                    theta_out = []\n",
    "                    for i in range(0, 23):\n",
    "                        theta_out.append(\n",
    "                            slim.fully_connected(\n",
    "                                poses[:, i, :, :],\n",
    "                                1,\n",
    "                                activation_fn=None,\n",
    "                                scope=\"pose_out_j%d\" % i))\n",
    "                    theta_out_all = tf.squeeze(tf.stack(theta_out, axis=1))\n",
    "\n",
    "                    # Do shape on it's own:\n",
    "                    shapes = slim.stack(\n",
    "                        shapes,\n",
    "                        slim.fully_connected, [10, 5],\n",
    "                        scope=\"shape_fc1\")\n",
    "                    shape_out = slim.fully_connected(\n",
    "                        shapes, 1, activation_fn=None, scope=\"shape_final\")\n",
    "                    \"\"\" Compute joint correlation prior!\"\"\"\n",
    "                    nz_feat = 1024\n",
    "                    poses_all = slim.flatten(poses, scope='vectorize')\n",
    "                    poses_all = slim.fully_connected(\n",
    "                        poses_all, nz_feat, scope=\"D_alljoints_fc1\")\n",
    "                    poses_all = slim.fully_connected(\n",
    "                        poses_all, nz_feat, scope=\"D_alljoints_fc2\")\n",
    "                    poses_all_out = slim.fully_connected(\n",
    "                        poses_all,\n",
    "                        1,\n",
    "                        activation_fn=None,\n",
    "                        scope=\"D_alljoints_out\")\n",
    "                    out = tf.concat([theta_out_all,\n",
    "                                     poses_all_out, shape_out], 1)\n",
    "\n",
    "            variables = tf.contrib.framework.get_variables(scope)\n",
    "            return out, variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All images are scaled to 224 × 224 preserving the aspect ratio such that the diagonal of the tight bounding box is roughly 150px. The images are randomly scaled, translated, and flipped. Mini-batch size is 64. When paired 3D supervision is employed each mini-batch is balanced such that it consists of half 2D and half 3D samples. All experiments use all datasets with paired 3D loss unless otherwise specified.\n",
    "\n",
    "The definition of the K = 23 joints in SMPL do not align perfectly with the common joint definitions used by these datasets. We se a regressor to obtain he ***14*** joints of Human3.6M from the reconstructed mesh. In addition, we also incorporate the 5 face keypoints from the MS COCO dataset. New keypoints can easily be incorporated with the mesh representation by specifying the corresponding vertex IDs. In total the reprojection error is computed over P = 19 keypoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the ResNet-50 network for encoding the image, pretrained on the ImageNet classification task. The ResNet output is average pooled, producing features $\\phi \\in R^{2048} $. \n",
    "\n",
    "The 3D regression module consists of two fully-connected layers with 1024 neurons each with a dropout layer in between, followed by a final layer of 85D neurons. We use T = 3 iterations for all of our experiments. \n",
    "\n",
    "The discriminator ***for the shape*** is two fully-connected layers with 10, 5, and 1 neurons. For pose, $\\theta$ is first converted to K many 3 × 3 rotation matrices via the ***Rodrigues formula***. Each rotation matrix is sent to a common embedding\n",
    "network of two fully-connected layers with 32 hidden neurons. Then the outputs are sent to K = 23 different discriminators that output 1-D values. The discriminator for overall pose distribution concatenates all K ∗ 32 representations through another two fully-connected layers of 1024 neurons each and finally outputs a 1D value. All layers use ReLU activations except the final layer. The learning rates of the encoder and the discriminator network are set to 1 × 10−5 and 1×10−4 respectively. We use the Adam solver and train for 55 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
