{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.contrib.layers.python.layers.initializers import variance_scaling_initializer\n",
    "\n",
    "import skimage.io as io\n",
    "import numpy as np\n",
    "from src.util import image as img_util\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import deepdish as dd\n",
    "from os.path import join, dirname\n",
    "\n",
    "from utilsTest import *\n",
    "\n",
    "import os.path as osp\n",
    "import os\n",
    "import sys\n",
    "curr_path = osp.dirname(os.getcwd())\n",
    "model_dir = osp.join(curr_path, '..', 'models')\n",
    "\n",
    "smpl_model_path = r'C:\\_Files\\MyProjects\\ASDS_3\\Photo_Wake-Up\\src\\HMR\\TensorFlow\\models\\neutral_smpl_with_cocoplus_reg.pkl'\n",
    "smpl_face_path = r'C:\\_Files\\MyProjects\\ASDS_3\\Photo_Wake-Up\\src\\HMR\\TensorFlow\\src\\tf_smpl\\smpl_faces.npy'\n",
    "\n",
    "from src.tf_smpl.batch_lbs import batch_rodrigues\n",
    "from src.tf_smpl.batch_smpl import SMPL\n",
    "from src.tf_smpl.projection import batch_orth_proj_idrot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing so the max image size is 224..\n"
     ]
    }
   ],
   "source": [
    "img_path = 'data/im1963.jpg'\n",
    "json_path = None\n",
    "input_img, proc_param, img = preprocess_image(img_path, json_path)\n",
    "input_img = np.expand_dims(input_img, 0)\n",
    "input_img = np.float32(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mean_param():\n",
    "    mean = np.zeros((1, 85))\n",
    "    # Initialize scale at 0.9\n",
    "    mean[0, 0] = 0.9\n",
    "    mean_path = join(\n",
    "        dirname(smpl_model_path), 'neutral_smpl_mean_params.h5')\n",
    "    mean_vals = dd.io.load(mean_path)\n",
    "\n",
    "    mean_pose = mean_vals['pose']\n",
    "    # Ignore the global rotation.\n",
    "    mean_pose[:3] = 0.\n",
    "    mean_shape = mean_vals['shape']\n",
    "\n",
    "    # This initializes the global pose to be up-right when projected\n",
    "    mean_pose[0] = np.pi\n",
    "\n",
    "    mean[0, 3:] = np.hstack((mean_pose, mean_shape))\n",
    "    mean = tf.constant(mean, tf.float32)\n",
    "    mean_var = tf.Variable(\n",
    "        mean, name=\"mean_param\", dtype=tf.float32, trainable=True)\n",
    "    E_var.append(mean_var)\n",
    "    init_mean = tf.tile(mean_var, [1, 1])\n",
    "    return init_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_3d_label = True\n",
    "num_stage = 3\n",
    "total_params = 85\n",
    "num_cam = 3\n",
    "num_theta = 72\n",
    "batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0227 23:34:23.694994  1456 deprecation_wrapper.py:119] From C:\\_Files\\MyProjects\\ASDS_3\\Photo_Wake-Up\\src\\HMR\\TensorFlow\\utilsTest.py:61: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0227 23:34:24.009151  1456 deprecation.py:323] From C:\\_Files\\MyProjects\\ASDS_3\\Photo_Wake-Up\\src\\HMR\\TensorFlow\\src\\tf_smpl\\batch_lbs.py:55: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1\n",
      "Reuse is on!\n",
      "Iteration 2\n",
      "Reuse is on!\n"
     ]
    }
   ],
   "source": [
    "img_enc_fn, threed_enc_fn = get_encoder_fn_separate('resnet_fc3_dropout')\n",
    "img_feat, E_var = img_enc_fn(input_img, is_training=False, reuse=False)\n",
    "smpl = SMPL(smpl_model_path)\n",
    "\n",
    "loss_kps = []\n",
    "if use_3d_label:\n",
    "    loss_3d_joints, loss_3d_params = [], []\n",
    "# For discriminator\n",
    "fake_rotations, fake_shapes = [], []\n",
    "# Start loop\n",
    "# 85D\n",
    "theta_prev = load_mean_param()\n",
    "\n",
    "# For visualizations\n",
    "all_verts = []\n",
    "all_pred_kps = []\n",
    "all_pred_cams = []\n",
    "all_delta_thetas = []\n",
    "all_theta_prev = []\n",
    "\n",
    "# Main IEF loop\n",
    "for i in np.arange(num_stage):\n",
    "    print('Iteration %d' % i)\n",
    "    # ---- Compute outputs\n",
    "    state = tf.concat([img_feat, theta_prev], 1)\n",
    "\n",
    "    if i == 0:\n",
    "        delta_theta, _ = threed_enc_fn(state, num_output= total_params, reuse=False)\n",
    "    else:\n",
    "        delta_theta, _ = threed_enc_fn(state, num_output= total_params, reuse=True)\n",
    "\n",
    "    # Compute new theta\n",
    "    theta_here = theta_prev + delta_theta\n",
    "    # cam = N x 3, pose N x self.num_theta, shape: N x 10\n",
    "    cams = theta_here[:, :num_cam]\n",
    "    poses = theta_here[:, num_cam:(num_cam + num_theta)]\n",
    "    shapes = theta_here[:, (num_cam + num_theta):]\n",
    "    # Rs_wglobal is Nx24x3x3 rotation matrices of poses\n",
    "    verts, Js, pred_Rs = smpl(shapes, poses, get_skin=True)\n",
    "    pred_kp = batch_orth_proj_idrot(Js, cams, name='proj2d_stage%d' % i)\n",
    "    # --- Compute losses:\n",
    "#     loss_kps.append(keypoint_loss(\n",
    "#         self.kp_loader, pred_kp))\n",
    "    pred_Rs = tf.reshape(pred_Rs, [-1, 24, 9])\n",
    "#     if self.use_3d_label:\n",
    "#         loss_poseshape, loss_joints = self.get_3d_loss(\n",
    "#             pred_Rs, shapes, Js)\n",
    "#         loss_3d_params.append(loss_poseshape)\n",
    "#         loss_3d_joints.append(loss_joints)\n",
    "\n",
    "    # Save pred_rotations for Discriminator\n",
    "    fake_rotations.append(pred_Rs[:, 1:, :])\n",
    "    fake_shapes.append(shapes)\n",
    "\n",
    "    # Finally update to end iteration.\n",
    "    theta_prev = theta_here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'strided_slice_2:0' shape=(1, 10) dtype=float32>,\n",
       " <tf.Tensor 'strided_slice_6:0' shape=(1, 10) dtype=float32>,\n",
       " <tf.Tensor 'strided_slice_10:0' shape=(1, 10) dtype=float32>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'strided_slice_3:0' shape=(1, 23, 9) dtype=float32>,\n",
       " <tf.Tensor 'strided_slice_7:0' shape=(1, 23, 9) dtype=float32>,\n",
       " <tf.Tensor 'strided_slice_11:0' shape=(1, 23, 9) dtype=float32>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Reshape_3:0' shape=(3, 23, 9) dtype=float32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_fake_rotations = tf.reshape(\n",
    "    tf.concat(fake_rotations, 0),\n",
    "    [batch_size * num_stage, -1, 9])\n",
    "all_fake_rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat_4:0' shape=(3, 10) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_fake_shapes = tf.concat(fake_shapes, 0)\n",
    "all_fake_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator_separable_rotations(\n",
    "        poses,\n",
    "        shapes,\n",
    "        weight_decay,\n",
    "):\n",
    "    \"\"\"\n",
    "    23 Discriminators on each joint + 1 for all joints + 1 for shape.\n",
    "    To share the params on rotations, this treats the 23 rotation matrices\n",
    "    as a \"vertical image\":\n",
    "    Do 1x1 conv, then send off to 23 independent classifiers.\n",
    "\n",
    "    Input:\n",
    "    - poses: N x 23 x 1 x 9, NHWC ALWAYS!!\n",
    "    - shapes: N x 10\n",
    "    - weight_decay: float\n",
    "\n",
    "    Outputs:\n",
    "    - prediction: N x (1+23) or N x (1+23+1) if do_joint is on.\n",
    "    - variables: tf variables\n",
    "    \"\"\"\n",
    "    data_format = \"NHWC\"\n",
    "    with tf.name_scope(\"Discriminator_sep_rotations\", [poses, shapes]):\n",
    "        with tf.variable_scope(\"D\") as scope:\n",
    "            with slim.arg_scope(\n",
    "                [slim.conv2d, slim.fully_connected],\n",
    "                    weights_regularizer=slim.l2_regularizer(weight_decay)):\n",
    "                with slim.arg_scope([slim.conv2d], data_format=data_format):\n",
    "                    poses = slim.conv2d(poses, 32, [1, 1], scope='D_conv1')\n",
    "                    poses = slim.conv2d(poses, 32, [1, 1], scope='D_conv2')\n",
    "                    theta_out = []\n",
    "                    for i in range(0, 23):\n",
    "                        theta_out.append(\n",
    "                            slim.fully_connected(\n",
    "                                poses[:, i, :, :],\n",
    "                                1,\n",
    "                                activation_fn=None,\n",
    "                                scope=\"pose_out_j%d\" % i))\n",
    "                    theta_out_all = tf.squeeze(tf.stack(theta_out, axis=1))\n",
    "\n",
    "                    # Do shape on it's own:\n",
    "                    shapes = slim.stack(\n",
    "                        shapes,\n",
    "                        slim.fully_connected, [10, 5],\n",
    "                        scope=\"shape_fc1\")\n",
    "                    shape_out = slim.fully_connected(\n",
    "                        shapes, 1, activation_fn=None, scope=\"shape_final\")\n",
    "                    \n",
    "                    \"\"\" Compute joint correlation prior!\"\"\"\n",
    "                    nz_feat = 1024\n",
    "                    poses_all = slim.flatten(poses, scope='vectorize')\n",
    "                    poses_all = slim.fully_connected(\n",
    "                        poses_all, nz_feat, scope=\"D_alljoints_fc1\")\n",
    "                    poses_all = slim.fully_connected(\n",
    "                        poses_all, nz_feat, scope=\"D_alljoints_fc2\")\n",
    "                    poses_all_out = slim.fully_connected(\n",
    "                        poses_all,\n",
    "                        1,\n",
    "                        activation_fn=None,\n",
    "                        scope=\"D_alljoints_out\")\n",
    "                    out = tf.concat([theta_out_all,\n",
    "                                     poses_all_out, shape_out], 1)\n",
    "\n",
    "            variables = tf.contrib.framework.get_variables(scope)\n",
    "            return out, variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_rodrigues(theta, name=None):\n",
    "    \"\"\"\n",
    "    Theta is N x 3\n",
    "    \"\"\"\n",
    "    # with tf.name_scope(name, \"batch_rodrigues\", [theta]):\n",
    "    with tf.name_scope(name, \"batch_rodrigues\"):\n",
    "        batch_size = theta.shape.as_list()[0]\n",
    "        \n",
    "        angle = tf.expand_dims(tf.norm(theta + 1e-8, axis=1), -1)\n",
    "        r = tf.expand_dims(tf.div(theta, angle), -1)\n",
    "\n",
    "        angle = tf.expand_dims(angle, -1)\n",
    "        cos = tf.cos(angle)\n",
    "        sin = tf.sin(angle)\n",
    "\n",
    "        outer = tf.matmul(r, r, transpose_b=True, name=\"outer\")\n",
    "\n",
    "        eyes = tf.tile(tf.expand_dims(tf.eye(3), 0), [batch_size, 1, 1])\n",
    "        R = cos * eyes + (1 - cos) * outer + sin * batch_skew(\n",
    "            r, batch_size=batch_size)\n",
    "        return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_discriminator(fake_rotations, fake_shapes):\n",
    "    # Compute the rotation matrices of \"rea\" pose.\n",
    "    # These guys are in 24 x 3.\n",
    "    real_rotations = batch_rodrigues(tf.reshape(pose_loader, [-1, 3]))\n",
    "    real_rotations = tf.reshape(real_rotations, [-1, 24, 9])\n",
    "    # Ignoring global rotation. N x 23*9\n",
    "    # The # of real rotation is B*num_stage so it's balanced.\n",
    "    real_rotations = real_rotations[:, 1:, :]\n",
    "    all_fake_rotations = tf.reshape(\n",
    "        tf.concat(fake_rotations, 0),\n",
    "        [batch_size * num_stage, -1, 9])\n",
    "    comb_rotations = tf.concat(\n",
    "        [real_rotations, all_fake_rotations], 0, name=\"combined_pose\")\n",
    "    comb_rotations = tf.expand_dims(comb_rotations, 2)\n",
    "\n",
    "    all_fake_shapes = tf.concat(fake_shapes, 0)\n",
    "    comb_shapes = tf.concat(\n",
    "        [shape_loader, all_fake_shapes], 0, name=\"combined_shape\")\n",
    "\n",
    "    disc_input = {\n",
    "        'weight_decay': d_wd,\n",
    "        'shapes': comb_shapes,\n",
    "        'poses': comb_rotations\n",
    "    }\n",
    "\n",
    "    d_out, D_var = Discriminator_separable_rotations(\n",
    "        **disc_input)\n",
    "\n",
    "    d_out_real, d_out_fake = tf.split(d_out, 2)\n",
    "    # Compute losses:\n",
    "    with tf.name_scope(\"comp_d_loss\"):\n",
    "        d_loss_real = tf.reduce_mean(\n",
    "            tf.reduce_sum((d_out_real - 1)**2, axis=1))\n",
    "        d_loss_fake = tf.reduce_mean(\n",
    "            tf.reduce_sum((d_out_fake)**2, axis=1))\n",
    "        # Encoder loss\n",
    "        e_loss_disc = tf.reduce_mean(\n",
    "            tf.reduce_sum((d_out_fake - 1)**2, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
