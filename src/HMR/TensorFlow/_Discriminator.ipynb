{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">Discriminator</div>\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "you can Find me on Github:\n",
    "> ###### [ GitHub](https://github.com/lev1khachatryan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reprojection loss encourages the network to produce a 3D body that explains the 2D joint locations, however  nthropometrically implausible 3D bodies or bodies with gross self-intersections may still minimize the reprojection loss. To regularize this, we use a discriminator network D that is trained to tell whether SMPL parameters correspond to a real body or not. We refer to this as an adversarial prior since the discriminator acts as a data-driven prior that guides the 3D inference.\n",
    "\n",
    "A further benefit of employing a rich, explicit 3D representation like SMPL is that we precisely know the meaning of the latent space. In particular SMPL has a factorized form that we can take advantage of to make the adversary more data efficient and stable to train. More concretely, we mirror the shape and pose decomposition of SMPL and train a discriminator for shape and pose independently. The pose is based on a kinematic tree, so we further decompose the pose discriminators and train one for each joint rotation. This amounts to learning the angle limits for each joint. In order to capture the joint distribution of the entire kinematic tree, we also learn a discriminator that takes in all the rotations.\n",
    "\n",
    "Since the input to each discriminator is very low dimensional (10-D for $\\beta$, 9-D for each joint and 9K-D for all joints), they can each be small networks, making them rather stable to train. All pose discriminators share a common feature space of rotation matrices and only the final classifiers are learned separately.\n",
    "\n",
    "In all we train K + 2 discriminators. Each discriminator $D_{i}$ outputs values between [0, 1], representing the probability that $\\theta$ came from the data. In practice we use the least square formulation for its stability. Let E represent the\n",
    "encoder including the image encoder and the 3D module.\n",
    "\n",
    "Then the adversarial loss function for the encoder is:\n",
    "\n",
    "<img src='assets/1.png'>\n",
    "\n",
    "and the objective for each discriminator is\n",
    "\n",
    "<img src='assets/2.png'>\n",
    "\n",
    "We optimize $E$ and all $D_{i}$'s jointly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "\n",
    "from tensorflow.contrib.layers.python.layers.initializers import variance_scaling_initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder_resnet(x, is_training=True, weight_decay=0.001, reuse=False):\n",
    "    \"\"\"\n",
    "    Resnet v2-50\n",
    "    Assumes input is [batch, height_in, width_in, channels]!!\n",
    "    Input:\n",
    "    - x: N x H x W x 3\n",
    "    - weight_decay: float\n",
    "    - reuse: bool->True if test\n",
    "\n",
    "    Outputs:\n",
    "    - cam: N x 3\n",
    "    - Pose vector: N x 72\n",
    "    - Shape vector: N x 10\n",
    "    - variables: tf variables\n",
    "    \"\"\"\n",
    "    from tensorflow.contrib.slim.python.slim.nets import resnet_v2\n",
    "    # with tf.name_scope(\"Encoder_resnet\", [x]):\n",
    "    with tf.name_scope(\"Encoder_resnet\"):\n",
    "        with slim.arg_scope(resnet_v2.resnet_arg_scope(weight_decay=weight_decay)):\n",
    "            net, end_points = resnet_v2.resnet_v2_50(\n",
    "                x,\n",
    "                num_classes=None,\n",
    "                is_training=is_training,\n",
    "                reuse=reuse,\n",
    "                scope='resnet_v2_50')\n",
    "            net = tf.squeeze(net, axis=[1, 2])\n",
    "    variables = tf.contrib.framework.get_variables('resnet_v2_50')\n",
    "    return net, variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encoder_fc3_dropout(x,\n",
    "                        num_output=85,\n",
    "                        is_training=True,\n",
    "                        reuse=False,\n",
    "                        name=\"3D_module\"):\n",
    "    \"\"\"\n",
    "    3D inference module. 3 MLP layers (last is the output)\n",
    "    With dropout  on first 2.\n",
    "    Input:\n",
    "    - x: N x [|img_feat|, |3D_param|]\n",
    "    - reuse: bool\n",
    "\n",
    "    Outputs:\n",
    "    - 3D params: N x num_output\n",
    "      if orthogonal: \n",
    "           either 85: (3 + 24*3 + 10) or 109 (3 + 24*4 + 10) for factored axis-angle representation\n",
    "      if perspective:\n",
    "          86: (f, tx, ty, tz) + 24*3 + 10, or 110 for factored axis-angle.\n",
    "    - variables: tf variables\n",
    "    \"\"\"\n",
    "    if reuse:\n",
    "        print('Reuse is on!')\n",
    "    with tf.variable_scope(name, reuse=reuse) as scope:\n",
    "        net = slim.fully_connected(x, 1024, scope='fc1')\n",
    "        net = slim.dropout(net, 0.5, is_training=is_training, scope='dropout1')\n",
    "        net = slim.fully_connected(net, 1024, scope='fc2')\n",
    "        net = slim.dropout(net, 0.5, is_training=is_training, scope='dropout2')\n",
    "        small_xavier = variance_scaling_initializer(\n",
    "            factor=.01, mode='FAN_AVG', uniform=True)\n",
    "        net = slim.fully_connected(\n",
    "            net,\n",
    "            num_output,\n",
    "            activation_fn=None,\n",
    "            weights_initializer=small_xavier,\n",
    "            scope='fc3')\n",
    "\n",
    "    variables = tf.contrib.framework.get_variables(scope)\n",
    "    return net, variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoder_fn_separate(model_type):\n",
    "    \"\"\"\n",
    "    Retrieves diff encoder fn for image and 3D\n",
    "    \"\"\"\n",
    "    encoder_fn = None\n",
    "    threed_fn = None\n",
    "    if 'resnet' in model_type:\n",
    "        encoder_fn = Encoder_resnet\n",
    "    else:\n",
    "        print('Unknown encoder %s!' % model_type)\n",
    "        exit(1)\n",
    "\n",
    "    if 'fc3_dropout' in model_type:\n",
    "        threed_fn = Encoder_fc3_dropout\n",
    "\n",
    "    if encoder_fn is None or threed_fn is None:\n",
    "        print('Dont know what encoder to use for %s' % model_type)\n",
    "        import ipdb\n",
    "        ipdb.set_trace()\n",
    "\n",
    "    return encoder_fn, threed_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_enc_fn, threed_enc_fn = get_encoder_fn_separate('resnet_fc3_dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.io as io\n",
    "import numpy as np\n",
    "from src.util import image as img_util\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path, json_path=None):\n",
    "    img = io.imread(img_path)\n",
    "    if img.shape[2] == 4:\n",
    "        img = img[:, :, :3]\n",
    "\n",
    "    if json_path is None:\n",
    "        if np.max(img.shape[:2]) != 224:\n",
    "            print('Resizing so the max image size is %d..' % 224)\n",
    "            scale = (float(224) / np.max(img.shape[:2]))\n",
    "        else:\n",
    "            scale = 1.\n",
    "        center = np.round(np.array(img.shape[:2]) / 2).astype(int)\n",
    "        # image center in (x,y)\n",
    "        center = center[::-1]\n",
    "    else:\n",
    "        scale, center = op_util.get_bbox(json_path)\n",
    "\n",
    "    crop, proc_param = img_util.scale_and_crop(img, scale, center, 224)\n",
    "\n",
    "    # Normalize image to [-1, 1]\n",
    "    crop = 2 * ((crop / 255.) - 0.5)\n",
    "\n",
    "    return crop, proc_param, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing so the max image size is 224..\n"
     ]
    }
   ],
   "source": [
    "img_path = 'data/im1963.jpg'\n",
    "json_path = None\n",
    "input_img, proc_param, img = preprocess_image(img_path, json_path)\n",
    "input_img = np.expand_dims(input_img, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = np.float32(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract image features.\n",
    "img_feat, E_var = img_enc_fn(input_img, is_training=False, reuse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import os\n",
    "import sys\n",
    "curr_path = osp.dirname(os.getcwd())\n",
    "model_dir = osp.join(curr_path, '..', 'models')\n",
    "\n",
    "# SMPL_MODEL_PATH = osp.join(model_dir, 'neutral_smpl_with_cocoplus_reg.pkl')\n",
    "# SMPL_FACE_PATH = osp.join(curr_path, '../src/tf_smpl', 'smpl_faces.npy')\n",
    "\n",
    "# smpl_model_path = SMPL_MODEL_PATH\n",
    "# smpl_face_path = SMPL_FACE_PATH\n",
    "\n",
    "smpl_model_path = r'C:\\_Files\\MyProjects\\ASDS_3\\Photo_Wake-Up\\src\\HMR\\TensorFlow\\models\\neutral_smpl_with_cocoplus_reg.pkl'\n",
    "smpl_face_path = r'C:\\_Files\\MyProjects\\ASDS_3\\Photo_Wake-Up\\src\\HMR\\TensorFlow\\src\\tf_smpl\\smpl_faces.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepdish as dd\n",
    "from os.path import join, dirname\n",
    "def load_mean_param():\n",
    "    mean = np.zeros((1, 85))\n",
    "    # Initialize scale at 0.9\n",
    "    mean[0, 0] = 0.9\n",
    "    mean_path = join(\n",
    "        dirname(smpl_model_path), 'neutral_smpl_mean_params.h5')\n",
    "    mean_vals = dd.io.load(mean_path)\n",
    "\n",
    "    mean_pose = mean_vals['pose']\n",
    "    # Ignore the global rotation.\n",
    "    mean_pose[:3] = 0.\n",
    "    mean_shape = mean_vals['shape']\n",
    "\n",
    "    # This initializes the global pose to be up-right when projected\n",
    "    mean_pose[0] = np.pi\n",
    "\n",
    "    mean[0, 3:] = np.hstack((mean_pose, mean_shape))\n",
    "    mean = tf.constant(mean, tf.float32)\n",
    "    mean_var = tf.Variable(\n",
    "        mean, name=\"mean_param\", dtype=tf.float32, trainable=True)\n",
    "    E_var.append(mean_var)\n",
    "    init_mean = tf.tile(mean_var, [1, 1])\n",
    "    return init_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.tf_smpl.batch_lbs import batch_rodrigues\n",
    "from src.tf_smpl.batch_smpl import SMPL\n",
    "from src.tf_smpl.projection import batch_orth_proj_idrot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl = SMPL(smpl_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0216 21:22:17.310510 16060 deprecation.py:323] From C:\\_Files\\MyProjects\\ASDS_3\\Photo_Wake-Up\\src\\HMR\\TensorFlow\\src\\tf_smpl\\batch_lbs.py:55: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "num_cam=3\n",
    "num_theta=72\n",
    "loss_kps = []\n",
    "# if self.use_3d_label:\n",
    "#     loss_3d_joints, loss_3d_params = [], []\n",
    "# For discriminator\n",
    "fake_rotations, fake_shapes = [], []\n",
    "# Start loop\n",
    "# 85D\n",
    "theta_prev = load_mean_param()\n",
    "\n",
    "# For visualizations\n",
    "all_verts = []\n",
    "all_pred_kps = []\n",
    "all_pred_cams = []\n",
    "all_delta_thetas = []\n",
    "all_theta_prev = []\n",
    "\n",
    "num_stage = 1\n",
    "\n",
    "# Main IEF loop\n",
    "for i in np.arange(num_stage):\n",
    "    print('Iteration %d' % i)\n",
    "    # ---- Compute outputs\n",
    "    state = tf.concat([img_feat, theta_prev], 1)\n",
    "\n",
    "    if i == 0:\n",
    "        delta_theta, threeD_var = threed_enc_fn(\n",
    "            state, num_output=85, reuse=False)\n",
    "        E_var.extend(threeD_var)\n",
    "    else:\n",
    "        delta_theta, _ = threed_enc_fn(\n",
    "            state, num_output=85, reuse=True)\n",
    "\n",
    "    # Compute new theta\n",
    "    theta_here = theta_prev + delta_theta\n",
    "    # cam = N x 3, pose N x self.num_theta, shape: N x 10\n",
    "    cams = theta_here[:, : num_cam]\n",
    "    poses = theta_here[:, num_cam:(num_cam + num_theta)]\n",
    "    shapes = theta_here[:, (num_cam + num_theta):]\n",
    "    \n",
    "    # Rs_wglobal is Nx24x3x3 rotation matrices of poses\n",
    "    verts, Js, pred_Rs = smpl(shapes, poses, get_skin=True)\n",
    "    pred_kp = batch_orth_proj_idrot(\n",
    "        Js, cams, name='proj2d_stage%d' % i)\n",
    "#     # --- Compute losses:\n",
    "#     loss_kps.append(self.e_loss_weight * self.keypoint_loss(\n",
    "#         self.kp_loader, pred_kp))\n",
    "    pred_Rs = tf.reshape(pred_Rs, [-1, 24, 9])\n",
    "#     if self.use_3d_label:\n",
    "#         loss_poseshape, loss_joints = self.get_3d_loss(\n",
    "#             pred_Rs, shapes, Js)\n",
    "#         loss_3d_params.append(loss_poseshape)\n",
    "#         loss_3d_joints.append(loss_joints)\n",
    "\n",
    "    # Save pred_rotations for Discriminator\n",
    "    fake_rotations.append(pred_Rs[:, 1:, :])\n",
    "    fake_shapes.append(shapes)\n",
    "\n",
    "#     # Save things for visualiations:\n",
    "#     self.all_verts.append(tf.gather(verts, self.show_these))\n",
    "#     self.all_pred_kps.append(tf.gather(pred_kp, self.show_these))\n",
    "#     self.all_pred_cams.append(tf.gather(cams, self.show_these))\n",
    "\n",
    "    # Finally update to end iteration.\n",
    "    theta_prev = theta_here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator_separable_rotations(\n",
    "        poses,\n",
    "        shapes,\n",
    "        weight_decay,\n",
    "):\n",
    "    \"\"\"\n",
    "    23 Discriminators on each joint + 1 for all joints + 1 for shape.\n",
    "    To share the params on rotations, this treats the 23 rotation matrices\n",
    "    as a \"vertical image\":\n",
    "    Do 1x1 conv, then send off to 23 independent classifiers.\n",
    "\n",
    "    Input:\n",
    "    - poses: N x 23 x 1 x 9, NHWC ALWAYS!!\n",
    "    - shapes: N x 10\n",
    "    - weight_decay: float\n",
    "\n",
    "    Outputs:\n",
    "    - prediction: N x (1+23) or N x (1+23+1) if do_joint is on.\n",
    "    - variables: tf variables\n",
    "    \"\"\"\n",
    "    data_format = \"NHWC\"\n",
    "    with tf.name_scope(\"Discriminator_sep_rotations\", [poses, shapes]):\n",
    "        with tf.variable_scope(\"D\") as scope:\n",
    "            with slim.arg_scope(\n",
    "                [slim.conv2d, slim.fully_connected],\n",
    "                    weights_regularizer=slim.l2_regularizer(weight_decay)):\n",
    "                with slim.arg_scope([slim.conv2d], data_format=data_format):\n",
    "                    poses = slim.conv2d(poses, 32, [1, 1], scope='D_conv1')\n",
    "                    poses = slim.conv2d(poses, 32, [1, 1], scope='D_conv2')\n",
    "                    theta_out = []\n",
    "                    for i in range(0, 23):\n",
    "                        theta_out.append(\n",
    "                            slim.fully_connected(\n",
    "                                poses[:, i, :, :],\n",
    "                                1,\n",
    "                                activation_fn=None,\n",
    "                                scope=\"pose_out_j%d\" % i))\n",
    "                    theta_out_all = tf.squeeze(tf.stack(theta_out, axis=1))\n",
    "\n",
    "                    # Do shape on it's own:\n",
    "                    shapes = slim.stack(\n",
    "                        shapes,\n",
    "                        slim.fully_connected, [10, 5],\n",
    "                        scope=\"shape_fc1\")\n",
    "                    shape_out = slim.fully_connected(\n",
    "                        shapes, 1, activation_fn=None, scope=\"shape_final\")\n",
    "                    \"\"\" Compute joint correlation prior!\"\"\"\n",
    "                    nz_feat = 1024\n",
    "                    poses_all = slim.flatten(poses, scope='vectorize')\n",
    "                    poses_all = slim.fully_connected(\n",
    "                        poses_all, nz_feat, scope=\"D_alljoints_fc1\")\n",
    "                    poses_all = slim.fully_connected(\n",
    "                        poses_all, nz_feat, scope=\"D_alljoints_fc2\")\n",
    "                    poses_all_out = slim.fully_connected(\n",
    "                        poses_all,\n",
    "                        1,\n",
    "                        activation_fn=None,\n",
    "                        scope=\"D_alljoints_out\")\n",
    "                    out = tf.concat([theta_out_all,\n",
    "                                     poses_all_out, shape_out], 1)\n",
    "\n",
    "            variables = tf.contrib.framework.get_variables(scope)\n",
    "            return out, variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_discriminator(fake_rotations, fake_shapes):\n",
    "    # Compute the rotation matrices of \"rea\" pose.\n",
    "    # These guys are in 24 x 3.\n",
    "    real_rotations = batch_rodrigues(tf.reshape(pose_loader, [-1, 3]))\n",
    "    real_rotations = tf.reshape(real_rotations, [-1, 24, 9])\n",
    "    # Ignoring global rotation. N x 23*9\n",
    "    # The # of real rotation is B*num_stage so it's balanced.\n",
    "    real_rotations = real_rotations[:, 1:, :]\n",
    "    all_fake_rotations = tf.reshape(\n",
    "        tf.concat(fake_rotations, 0),\n",
    "        [batch_size * num_stage, -1, 9])\n",
    "    comb_rotations = tf.concat(\n",
    "        [real_rotations, all_fake_rotations], 0, name=\"combined_pose\")\n",
    "\n",
    "    comb_rotations = tf.expand_dims(comb_rotations, 2)\n",
    "    all_fake_shapes = tf.concat(fake_shapes, 0)\n",
    "    comb_shapes = tf.concat(\n",
    "        [shape_loader, all_fake_shapes], 0, name=\"combined_shape\")\n",
    "\n",
    "    disc_input = {\n",
    "        'weight_decay': d_wd,\n",
    "        'shapes': comb_shapes,\n",
    "        'poses': comb_rotations\n",
    "    }\n",
    "\n",
    "    d_out, D_var = Discriminator_separable_rotations(\n",
    "        **disc_input)\n",
    "\n",
    "    d_out_real, d_out_fake = tf.split(d_out, 2)\n",
    "    # Compute losses:\n",
    "    with tf.name_scope(\"comp_d_loss\"):\n",
    "        d_loss_real = tf.reduce_mean(\n",
    "            tf.reduce_sum((d_out_real - 1)**2, axis=1))\n",
    "        d_loss_fake = tf.reduce_mean(\n",
    "            tf.reduce_sum((d_out_fake)**2, axis=1))\n",
    "        # Encoder loss\n",
    "        e_loss_disc = tf.reduce_mean(\n",
    "            tf.reduce_sum((d_out_fake - 1)**2, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'strided_slice_2:0' shape=(1, 10) dtype=float32>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'strided_slice_3:0' shape=(1, 23, 9) dtype=float32>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_rotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup_discriminator(fake_rotations, fake_shapes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
