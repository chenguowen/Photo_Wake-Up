{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div align=\"center\">PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization</div>\n",
    "---------------------------------------------------------------------\n",
    "\n",
    "you can Find me on Github:\n",
    "> ###### [ GitHub](https://github.com/lev1khachatryan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***PIFu is a memory efficient and spatiallyaligned 3D representation for 3D surfaces.*** An implicit function defines a surface as a level set of a function f, e.g. $f(X) = 0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In mathematics, a ***level set*** of a real-valued function f of n real variables is a set of the form $L_c(f) = \\{ (x_1, x_2, ..., x_n) | f(x_1, x_2, ..., x_n) = c \\}$, that is, a set where the function takes on a given constant value c. When the number of variables is two, a level set is generically a curve, called a ***level curve***, ***contour line***, or ***isoline***. So a level curve is the set of all real-valued solutions of an equation in two variables x1 and x2. When n = 3, a level set is called a ***level surface*** or ***isosurface***, and for higher values of n the level set is a level ***hypersurface***. So a level surface is the set of all real-valued roots of an equation in three variables x1, x2 and x3, and a level hypersurface is the set of all real-valued roots of an equation in n (n > 3) variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proposed pixel-aligned implicit function consists of a ***fully convolutional image encoder g*** and a ***continuous\n",
    "implicit function f represented by multi-layer perceptrons (MLPs)***, where the surface is defined as a level set of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">$f(F(x), z(X)) = s : s \\in R \\tag{1}$</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where \n",
    "\n",
    "* for 3D point $X, x = \\pi(X)$ is its 2D projection, \n",
    " \n",
    " \n",
    "* $z(X)$ is the depth value in the ***camera coordinate space***, \n",
    " \n",
    " \n",
    "* $F(x) = g(I(x))$ is the image feature at x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When you take a photograph, you need to move and rotate the camera to adjust the viewpoint. So in a way, when you transform a camera (by translating and rotating it — note that scaling a camera doesn't make much sense), what you actually do is transforming a local coordinate system which implicitly represents the transformations applied to that camera. In CG (computer graphics) we call this a ***camera coordinate*** system. <img src='assets/1.png'> A coordinate system $[i, j, k]$ centered at the pinhole $O$ such that the axis $k$ is perpendicular to the image plane and points toward it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume a ***weak-perspective camera***, but extending to perspective cameras is straightforward. Note that we obtain the pixelaligned feature F(x) using ***bilinear sampling***, because the 2D projection of X is defined in a continuous space rather\n",
    "than a discrete one (i.e., pixel)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ***3D projection*** or graphical projection maps points in 3D onto a 2D. We can represent the relationship between a point $P$ in 3D space and its $P'$ image coordinates by a matrix vector relationship: $P' = \\begin{bmatrix}\n",
    "       \\alpha & 0 & c_x & 0     \\\\[0.5em]\n",
    "       0 & \\beta & c_y &  0     \\\\[0.5em]\n",
    "       0 & 0 & 1 &  0           \\\\[0.5em]\n",
    "     \\end{bmatrix} P = MP = \\begin{bmatrix}\n",
    "       \\alpha & 0 & c_x     \\\\[0.5em]\n",
    "       0 & \\beta & c_y      \\\\[0.5em]\n",
    "       0 & 0 & 1           \\\\[0.5em]\n",
    "     \\end{bmatrix} \\;[I,\\; 0]\\; P = K \\;[I,\\; 0]\\; P$ and  $P = \\begin{bmatrix}\n",
    "       R & T     \\\\[0.5em]\n",
    "       0 & 1     \\\\[0.5em]\n",
    "     \\end{bmatrix} P_w$, so $P'= K \\;[R\\; T]\\; P_w$\n",
    "     \n",
    "> In the ***weak perspective model***, points are first projected to the camera cordinate space (reference plane) using orthogonal projection and then projected to the image plane using a projective transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <div align=\"center\">Single-view Surface Reconstruction</div>\n",
    "---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For surface reconstruction, we represent the ground truth surface as a ***0.5 level-set of a continuous 3D occupancy field***:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">$f_v^{*}(X) = \\begin{cases} 1, & \\mbox{if } X\\mbox{ is inside mesh surface} \\\\ 0, & \\mbox{otherwise} \\end{cases} \\tag{2}$</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train a pixel-aligned implicit function (PIFu) $f_v$ by minimizing the average of mean squared error:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">$L_V = 1/n \\sum_{i=1}^{n}|f_v(F_V(x_i), z(X_i)) - f_v^{*}(X_i)|^2 \\tag{3}$</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "\n",
    "* $X_i \\in R^3 $ , \n",
    "\n",
    "\n",
    "* $F_V(x) = g(I(x))$ is the image feature from the image encoder $g$ at $x = \\pi(X)$ and \n",
    "\n",
    "\n",
    "* $n$ is the number of sampled points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a pair of an input image and the corresponding 3D mesh that is ***spatially aligned*** with the input image, the parameters of the image encoder $g$ and PIFu $f_v$ are jointly updated by minimizing Eq. 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In image fusion, ***spatial alignment*** defined as the process of geometrically aligning two or more images of the same\n",
    "scene acquired at different times (multi-temporal fusion), or with different sensors (multi-modal fusion), or from different viewpoints (multi-view fusion). It is a crucial pre-processing operation in image fusion and its accuracy is a major factor in\n",
    "determining the quality of the output image. Suppose we have two input images, $A$ and $B$, our task is to find the transformation $T$ which \"optimally\" maps spatial locations in the image $B$ to the corresponding spatial locations in the image $A$. If $(u,v)$ denotes a pixel location in the reference image $A$ and $(x,y)$ denotes a pixel location in the floating image $B$, then the transformation $T$ represents a mapping of every pixel location $(x,y)$ in $B$ into the corresponding location $(u',v')$ in $A$.\n",
    "\n",
    "> Let $B'$ be the corresponding tranformed $B$ image. The image $B'$ is only defined at the points $(u',v')$, where by definition, $B'(u',v') = B(x,y)$. In order to convert $B'(u',v')$ into a digital image which is defined at the same pixel locations as $A$ we apply an ***interpolation/resampling*** operation to $B'(u',v')$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As Bansal demonstrate for semantic segmentation, training an image encoder with a subset of pixels does not hurt convergence compared with training with all the pixels. During inference, we densely sample the probability field over the 3D space and extract the iso-surface of the probability field at threshold 0.5 using the ***Marching Cube*** algorithm. This implicit surface representation is suitable for detailed objects with arbitrary topology. Aside from PIFu’s expressiveness and memoryefficiency, we develop a spatial sampling strategy that is critical for achieving high-fidelity inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The ***Marching cubes algorithm*** can be described as follows: Given an object, a test to determine whether an arbitrary point is within the object, and bounds within which the object exists: Divide the space within the bounds into an arbitrary number of cubes.  Test the corners of every cube for whether they are inside the object.  *For every cube where some corners are inside and some corners are outside the object, the surface must pass through that cube*, intersecting the edges of the cube in between corners of opposite classification.  Draw a surface within each cube connecting these intersections.  You have your object.\n",
    "\n",
    "> To understand how the Marching Cubes algorithm works, let's take a 2D case and what might be called the ***Marching Squares algorithm***. \n",
    "\n",
    "> The object   | Divide into squares    | Red for in, blue for out  | Between inside and outside  | Approximation  |\n",
    "> :----------------:|:-----------------:|:-----------------:|:-----------------:|:---------------:\n",
    "> ![](assets/2.jpg) | ![](assets/3.jpg) | ![](assets/4.jpg) | ![](assets/5.jpg) | ![](assets/6.jpg)\n",
    "\n",
    "> Implementing the algorithm in 3D works much the same as it did in 2D.  For slice data like the Visible Human Male dataset, you stack the slices in 3D, knowing each slice is 1mm or 3 pixels appart.  In order to be able to test the vertices of each cube, you must choose your cube size to align with the slices, either using 1mm cubes (or rectangles 1mm high and 1/3rd of a mm thick since pixels are only 1/3rd of a mm wide), or some multiple of 1mm cubes (ie 2mm or 3 mm) so that each horizontal side of your cubes falls on the plane of a slice.  You then can test each vertex by going to the masked slices corresponding to each cube's top and bottom z values.  You now have a bunch of cubes with labeled corners.  For each cube, you know the surface intersects the cube along the edges in between corners of opposing classifications.  Each cube should look something like this:\n",
    "\n",
    "> The object   | Divide into squares    |\n",
    "> :----------------:|:-----------------:\n",
    "> ![](assets/7.jpg) | ![](assets/8.jpg)\n",
    "\n",
    "> For more details please follow the video [link](https://www.youtube.com/watch?v=B_xk71YopsA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
